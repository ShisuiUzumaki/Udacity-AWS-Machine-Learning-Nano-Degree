{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Dog Breed Image Classification\n",
    "\n",
    "This notebook lists all the steps that you need to complete the complete this project. You will need to complete all the TODOs in this notebook as well as in the README and the two python scripts included with the starter code.\n",
    "\n",
    "\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of these the TODO's and use more than one TODO code cell to do all your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting smdebug\n",
      "  Using cached smdebug-1.0.12-py2.py3-none-any.whl (270 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from smdebug) (3.19.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from smdebug) (20.1)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /opt/conda/lib/python3.7/site-packages (from smdebug) (1.20.23)\n",
      "Collecting pyinstrument==3.4.2\n",
      "  Using cached pyinstrument-3.4.2-py2.py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from smdebug) (1.20.3)\n",
      "Collecting pyinstrument-cext>=0.2.2\n",
      "  Using cached pyinstrument_cext-0.2.4-cp37-cp37m-manylinux2010_x86_64.whl (20 kB)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (1.23.23)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (0.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->smdebug) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->smdebug) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.10.32->smdebug) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.10.32->smdebug) (1.26.7)\n",
      "Installing collected packages: pyinstrument-cext, pyinstrument, smdebug\n",
      "Successfully installed pyinstrument-3.4.2 pyinstrument-cext-0.2.4 smdebug-1.0.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO: Install any packages that you might need\n",
    "# For instance, you will need the smdebug package\n",
    "!pip install smdebug\n",
    "# !pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import any packages that you might need\n",
    "# For instance you will need Boto3 and Sagemaker\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "TODO: Explain what dataset you are using for this project. Maybe even give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understand of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-03 05:46:19--  https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.112.240\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.112.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1132023110 (1.1G) [application/zip]\n",
      "Saving to: ‘dogImages.zip’\n",
      "\n",
      "dogImages.zip       100%[===================>]   1.05G  41.1MB/s    in 31s     \n",
      "\n",
      "2021-12-03 05:46:52 (35.0 MB/s) - ‘dogImages.zip’ saved [1132023110/1132023110]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Fetch and upload the data to AWS S3\n",
    "\n",
    "# Command to download and unzip data\n",
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Bucket: sagemaker-studio-mb7oqzoi7lj\n",
      "AWS Region: us-east-1\n",
      "RoleArn: arn:aws:iam::015775941522:role/service-role/AmazonSageMaker-ExecutionRole-20211202T085370\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "bucket = 'sagemaker-studio-mb7oqzoi7lj' ## TODO: fill in\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "\n",
    "region = boto3.Session().region_name ## TODO: fill in\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = sagemaker.get_execution_role() ## TODO: fill in\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"dogImages\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#TODO: Import your dependencies.\u001b[39;49;00m\n",
      "\u001b[37m#For instance, below are some dependencies you might need if you are using Pytorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmodels\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtransforms\u001b[39;49;00m\n",
      "\u001b[37m# import smdebug.pytorch as smd\u001b[39;49;00m\n",
      "\u001b[37m# from smdebug import modes\u001b[39;49;00m\n",
      "\u001b[37m# from smdebug.profiler.utils import str2bool\u001b[39;49;00m\n",
      "\u001b[37m# from smdebug.pytorch import get_hook\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[37m# import boto3\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ImageFile\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\n",
      "ImageFile.LOAD_TRUNCATED_IMAGES = \u001b[34mTrue\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, criterion, hook):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Complete this function that can take a model and a \u001b[39;49;00m\n",
      "\u001b[33m          testing data loader and will get the test accuray/loss of the model\u001b[39;49;00m\n",
      "\u001b[33m          Remember to include any debugging/profiling hooks that you might need\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model.eval()\n",
      "    running_loss=\u001b[34m0\u001b[39;49;00m\n",
      "    running_corrects=\u001b[34m0\u001b[39;49;00m\n",
      "\u001b[37m#     hook.set_mode(smd.modes.EVAL)\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m inputs, labels \u001b[35min\u001b[39;49;00m test_loader:\n",
      "        inputs=inputs.to(device)\n",
      "        labels=labels.to(device)\n",
      "        outputs=model(inputs)\n",
      "        loss=criterion(outputs, labels)\n",
      "        _, preds = torch.max(outputs, \u001b[34m1\u001b[39;49;00m)\n",
      "        running_loss += loss.item() * inputs.size(\u001b[34m0\u001b[39;49;00m)\n",
      "        running_corrects += torch.sum(preds == labels.data).item()\n",
      "\n",
      "    total_loss = running_loss / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    total_acc = running_corrects/ \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTesting Accuracy: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33m100*total_acc}, Testing Loss: \u001b[39;49;00m\u001b[33m{total_loss}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(model, train_loader, validation_loader, criterion, optimizer, hook, epochs=\u001b[34m2\u001b[39;49;00m):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Complete this function that can take a model and\u001b[39;49;00m\n",
      "\u001b[33m          data loaders for training and will get train the model\u001b[39;49;00m\n",
      "\u001b[33m          Remember to include any debugging/profiling hooks that you might need\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "\u001b[37m#     hook.set_mode(smd.modes.TRAIN)\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    best_loss=\u001b[34m1e6\u001b[39;49;00m\n",
      "    image_dataset={\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:train_loader, \u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:validation_loader}\n",
      "    loss_counter=\u001b[34m0\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(epochs):\n",
      "        \u001b[34mfor\u001b[39;49;00m phase \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{epoch}\u001b[39;49;00m\u001b[33m, Phase \u001b[39;49;00m\u001b[33m{phase}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[34mif\u001b[39;49;00m phase==\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                model.train()\n",
      "            \u001b[34melse\u001b[39;49;00m:\n",
      "                model.eval()\n",
      "            running_loss = \u001b[34m0.0\u001b[39;49;00m\n",
      "            running_corrects = \u001b[34m0\u001b[39;49;00m\n",
      "            running_samples=\u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "            \u001b[34mfor\u001b[39;49;00m step, (inputs, labels) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(image_dataset[phase]):\n",
      "                inputs=inputs.to(device)\n",
      "                labels=labels.to(device)\n",
      "                outputs = model(inputs)\n",
      "                loss = criterion(outputs, labels)\n",
      "\n",
      "                \u001b[34mif\u001b[39;49;00m phase==\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                    optimizer.zero_grad()\n",
      "                    loss.backward()\n",
      "                    optimizer.step()\n",
      "\n",
      "                _, preds = torch.max(outputs, \u001b[34m1\u001b[39;49;00m)\n",
      "                running_loss += loss.item() * inputs.size(\u001b[34m0\u001b[39;49;00m)\n",
      "                running_corrects += torch.sum(preds == labels.data).item()\n",
      "                running_samples+=\u001b[36mlen\u001b[39;49;00m(inputs)\n",
      "                \u001b[34mif\u001b[39;49;00m running_samples % \u001b[34m2000\u001b[39;49;00m  == \u001b[34m0\u001b[39;49;00m:\n",
      "                    accuracy = running_corrects/running_samples\n",
      "                    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mImages [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.2f}\u001b[39;49;00m\u001b[33m Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.2f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "                            running_samples,\n",
      "                            \u001b[36mlen\u001b[39;49;00m(image_dataset[phase].dataset),\n",
      "                            \u001b[34m100.0\u001b[39;49;00m * (running_samples / \u001b[36mlen\u001b[39;49;00m(image_dataset[phase].dataset)),\n",
      "                            loss.item(),\n",
      "                            running_corrects,\n",
      "                            running_samples,\n",
      "                            \u001b[34m100.0\u001b[39;49;00m*accuracy,\n",
      "                        )\n",
      "                    )\n",
      "\n",
      "            epoch_loss = running_loss / running_samples\n",
      "            epoch_acc = running_corrects / running_samples\n",
      "            \n",
      "            \u001b[34mif\u001b[39;49;00m phase==\u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "                \u001b[34mif\u001b[39;49;00m epoch_loss<best_loss:\n",
      "                    best_loss=epoch_loss\n",
      "                \u001b[34melse\u001b[39;49;00m:\n",
      "                    loss_counter+=\u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m loss_counter==\u001b[34m1\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mnet\u001b[39;49;00m():\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Complete this function that initializes your model\u001b[39;49;00m\n",
      "\u001b[33m          Remember to use a pretrained model\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    model = models.resnet18(pretrained=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        param.requires_grad = \u001b[34mFalse\u001b[39;49;00m   \n",
      "\n",
      "    num_features=model.fc.in_features\n",
      "    model.fc = nn.Sequential(nn.Linear(num_features, \u001b[34m133\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_data_loaders\u001b[39;49;00m(data_dir, ctransform, batch_size):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    This is an optional function that you may or may not need to implement\u001b[39;49;00m\n",
      "\u001b[33m    depending on whether you need to use data loaders or not\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    data = torchvision.datasets.ImageFolder(root=data_dir, transform=ctransform)\n",
      "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m loader\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = resnet50(pretrained=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(args):\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Initialize a model by calling the net function\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    model=net()\n",
      "    \n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Create your loss and optimizer\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    loss_criterion = nn.CrossEntropyLoss()\n",
      "    optimizer = optim.Adam(model.fc.parameters(), lr=args.lr)\n",
      "    \n",
      "    \u001b[37m# ======================================================#\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Register the SMDebug hook to save output tensors. #\u001b[39;49;00m\n",
      "    \u001b[37m# ======================================================#\u001b[39;49;00m\n",
      "\u001b[37m#     hook = get_hook(create_if_not_exists=True)\u001b[39;49;00m\n",
      "    hook = \u001b[34mNone\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Call the train function to start training your model\u001b[39;49;00m\n",
      "\u001b[33m    Remember that you will need to set up a way to get training data from S3\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    train_dir, val_dir = args.train_data_dir, args.val_data_dir\n",
      "    batch_size = args.batch_size\n",
      "    epochs = args.epochs\n",
      "    train_transform = transforms.Compose([transforms.RandomRotation(\u001b[34m30\u001b[39;49;00m),\n",
      "                                            transforms.RandomResizedCrop(\u001b[34m224\u001b[39;49;00m),\n",
      "                                            transforms.RandomHorizontalFlip(),\n",
      "                                            transforms.ToTensor(),\n",
      "\u001b[37m#                                             transforms.Normalize([0.485, 0.456, 0.406], \u001b[39;49;00m\n",
      "\u001b[37m#                                                                     [0.229, 0.224, 0.225])\u001b[39;49;00m\n",
      "                                         ])\n",
      "    train_loader = create_data_loaders(train_dir, train_transform, batch_size)\n",
      "    valid_transforms = transforms.Compose([transforms.Resize(\u001b[34m256\u001b[39;49;00m),\n",
      "                                            transforms.CenterCrop(\u001b[34m224\u001b[39;49;00m),\n",
      "                                            transforms.ToTensor(),\n",
      "\u001b[37m#                                             transforms.Normalize([0.485, 0.456, 0.406], \u001b[39;49;00m\n",
      "\u001b[37m#                                                                 [0.229, 0.224, 0.225])\u001b[39;49;00m\n",
      "                                          ])\n",
      "    validation_loader = create_data_loaders(val_dir, valid_transforms ,batch_size)\n",
      "    model=train(model, train_loader, validation_loader, loss_criterion, optimizer, hook, epochs)\n",
      "    \n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Test the model to see its accuracy\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    test_dir = args.test_data_dir\n",
      "    test_transforms = transforms.Compose([transforms.Resize(\u001b[34m255\u001b[39;49;00m),\n",
      "                                            transforms.CenterCrop(\u001b[34m224\u001b[39;49;00m),\n",
      "                                            transforms.ToTensor(),\n",
      "\u001b[37m#                                             transforms.Normalize([0.485, 0.456, 0.406], \u001b[39;49;00m\n",
      "\u001b[37m#                                                                 [0.229, 0.224, 0.225])\u001b[39;49;00m\n",
      "                                         ])\n",
      "    test_loader = create_data_loaders(test_dir, test_transforms, batch_size)\n",
      "    test(model, test_loader, loss_criterion, hook)\n",
      "    \n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Save the trained model\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.state_dict(), f)\n",
      "        \n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser=argparse.ArgumentParser()\n",
      "    \u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33m    TODO: Specify all the hyperparameters you need to use to train your model.\u001b[39;49;00m\n",
      "\u001b[33m    '''\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mPyTorch dog images classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m32\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m2\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 14)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.003\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 1.0)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--train-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mTDD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining data directory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mEDD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mTest data directory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--val-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VAL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mVDD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mTest data directory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    args=parser.parse_args()\n",
      "        \n",
      "    main(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "**TODO:** This is the part where you will finetune a pretrained model with hyperparameter tuning. Remember that you have to tune a minimum of two hyperparameters. However you are encouraged to tune more. You are also encouraged to explain why you chose to tune those particular hyperparameters and the ranges.\n",
    "\n",
    "**Note:** You will need to use the `hpo.py` script to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Declare your HP ranges, metrics etc.\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001,0.01),\n",
    "#     \"epochs\": IntegerParameter(2, 16),\n",
    "    \"batch-size\": CategoricalParameter([16, 32, 64]),\n",
    "}\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create estimators for your HPs\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"hpo.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=5,\n",
    "    instance_type=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=5,\n",
    "    objective_type=objective_type,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_uri = f's3://sagemaker-studio-mb7oqzoi7lj/dogImages'\n",
    "input_paths = {\n",
    "    'train': s3_bucket_uri+'/train',       \n",
    "    'test': s3_bucket_uri+'/test',      \n",
    "    'val': s3_bucket_uri+'/valid'\n",
    "}\n",
    "_job_name = \"DogImage-Breed-Classification-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fit your HP Tuner\\\n",
    "tuner.fit(inputs=input_paths,job_name=_job_name, wait=True) # TODO: Remember to include your data channelsjob_name="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner.attach(_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-12-15 11:45:52 Starting - Preparing the instances for training\n",
      "2021-12-15 11:45:52 Downloading - Downloading input data\n",
      "2021-12-15 11:45:52 Training - Training image download completed. Training in progress.\n",
      "2021-12-15 11:45:52 Uploading - Uploading generated training model\n",
      "2021-12-15 11:45:52 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get the best estimators and the best HPs\n",
    "\n",
    "best_estimator = tuner.best_estimator() #TODO\n",
    "\n",
    "#Get the hyperparameters of the best trained model\n",
    "bh = best_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tuning_objective_metric': '\"average test loss\"',\n",
       " 'batch-size': '\"64\"',\n",
       " 'lr': '0.001295621162883104',\n",
       " 'sagemaker_container_log_level': '20',\n",
       " 'sagemaker_estimator_class_name': '\"PyTorch\"',\n",
       " 'sagemaker_estimator_module': '\"sagemaker.pytorch.estimator\"',\n",
       " 'sagemaker_job_name': '\"DogImage-Breed-Classification-5\"',\n",
       " 'sagemaker_program': '\"hpo.py\"',\n",
       " 'sagemaker_region': '\"us-east-1\"',\n",
       " 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-015775941522/DogImage-Breed-Classification-5/source/sourcedir.tar.gz\"'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 0.001295621162883104)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "batch_size = int(re.search(r'\\d+', bh['batch-size']).group())\n",
    "lr = float(bh['lr'])\n",
    "print((batch_size, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Profiling and Debugging\n",
    "TODO: Using the best hyperparameters, create and finetune a new model\n",
    "\n",
    "**Note:** You will need to use the `train_model.py` script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and fit an estimator\n",
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    DebuggerHookConfig,\n",
    "    CollectionConfig,\n",
    "    rule_configs,\n",
    ")\n",
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "rules = [\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing(), rule_parameters={ \"tensor_regex\": \"CrossEntropyLoss_output_0\", \"mode\": \"TRAIN\"})\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "\n",
    "collection_configs=[\n",
    "    CollectionConfig(\n",
    "                name=\"CrossEntropyLoss_output_0\",\n",
    "                parameters={\n",
    "                        \"include_regex\": \"CrossEntropyLoss_output_0\", \n",
    "                        \"train.save_interval\": \"100\",\n",
    "                        \"eval.save_interval\": \"10\"\n",
    "                }\n",
    "    )\n",
    "]\n",
    "hook_config = DebuggerHookConfig(\n",
    "    collection_configs=collection_configs\n",
    ")\n",
    "hyperparameters = {\"batch-size\": batch_size, \"lr\": lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 07:15:26 Starting - Starting the training job...\n",
      "2021-12-20 07:15:29 Starting - Launching requested ML instancesVanishingGradient: InProgress\n",
      "Overfit: InProgress\n",
      "Overtraining: InProgress\n",
      "PoorWeightInitialization: InProgress\n",
      "LossNotDecreasing: InProgress\n",
      "LowGPUUtilization: InProgress\n",
      "ProfilerReport: InProgress\n",
      "......\n",
      "2021-12-20 07:16:42 Starting - Preparing the instances for training......\n",
      "2021-12-20 07:17:55 Downloading - Downloading input data...............\n",
      "2021-12-20 07:20:24 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:19,212 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:19,216 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:19,233 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:19,242 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:35,029 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:35,042 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:35,053 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-20 07:20:35,062 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"lr\": 0.001295621162883104\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_model\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_model.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"lr\":0.001295621162883104}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_model.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_model\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"lr\":0.001295621162883104},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/source/sourcedir.tar.gz\",\"module_name\":\"train_model\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_model.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--lr\",\"0.001295621162883104\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001295621162883104\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_model.py --batch-size 64 --lr 0.001295621162883104\u001b[0m\n",
      "\u001b[34mEpoch 0, Phase train\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:37.465 algo-1:29 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:37.845 algo-1:29 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:37.847 algo-1:29 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:37.848 algo-1:29 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:37.850 algo-1:29 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:37.850 algo-1:29 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:39.005 algo-1:29 INFO hook.py:591] name:fc.0.weight count_params:68096\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:39.006 algo-1:29 INFO hook.py:591] name:fc.0.bias count_params:133\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:39.006 algo-1:29 INFO hook.py:593] Total Trainable Params: 68229\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:39.007 algo-1:29 INFO hook.py:425] Monitoring the collections: gradients, relu_input, CrossEntropyLoss_output_0, losses\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:39.008 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/prestepzero-*-start-1639984837846180.5_global-0-stepstart-1639984839008063.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:39.024 algo-1:29 INFO hook.py:488] Hook is writing from the hook with pid: 29\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:45.076 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-0-stepstart-1639984839015366.5_global-0-forwardpassend-1639984845076115.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:45.890 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-0-forwardpassend-1639984845082170.5_global-1-stepstart-1639984845889853.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:20:59.525 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-1-stepstart-1639984845894238.0_global-1-forwardpassend-1639984859524711.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:00.419 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-1-forwardpassend-1639984859527131.0_global-2-stepstart-1639984860418178.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:13.029 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-2-stepstart-1639984860423957.2_global-2-forwardpassend-1639984873029365.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:13.910 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-2-forwardpassend-1639984873031198.2_global-3-stepstart-1639984873909148.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:25.617 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-3-stepstart-1639984873913564.5_global-3-forwardpassend-1639984885617378.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:26.655 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-3-forwardpassend-1639984885619288.2_global-4-stepstart-1639984886654680.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:38.994 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-4-stepstart-1639984886658341.8_global-4-forwardpassend-1639984898994201.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:40.077 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-4-forwardpassend-1639984898996250.5_global-5-stepstart-1639984900077209.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:52.327 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-5-stepstart-1639984900083277.2_global-5-forwardpassend-1639984912327451.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:21:53.371 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-5-forwardpassend-1639984912329435.2_global-6-stepstart-1639984913368153.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:06.437 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-6-stepstart-1639984913374924.2_global-6-forwardpassend-1639984926437413.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:07.184 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-6-forwardpassend-1639984926439399.2_global-7-stepstart-1639984927183867.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:19.660 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-7-stepstart-1639984927188174.5_global-7-forwardpassend-1639984939659924.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:20.620 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-7-forwardpassend-1639984939662135.0_global-8-stepstart-1639984940617165.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:32.526 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-8-stepstart-1639984940623384.0_global-8-forwardpassend-1639984952526056.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:33.625 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-8-forwardpassend-1639984952529290.0_global-9-stepstart-1639984953623449.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:45.777 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-9-stepstart-1639984953629246.0_global-9-forwardpassend-1639984965776876.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2021-12-20 07:22:46.809 algo-1:29 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/29-algo-1/global-9-forwardpassend-1639984965778702.0_global-10-stepstart-1639984966808643.2/python_stats.\u001b[0m\n",
      "VanishingGradient: InProgress\n",
      "Overfit: InProgress\n",
      "Overtraining: InProgress\n",
      "PoorWeightInitialization: IssuesFound\n",
      "LossNotDecreasing: InProgress\n",
      "\u001b[34mEpoch 0, Phase valid\u001b[0m\n",
      "\u001b[34mEpoch 1, Phase train\u001b[0m\n",
      "\u001b[34mEpoch 1, Phase valid\u001b[0m\n",
      "\u001b[34mTesting Accuracy: 73.68421052631578, Testing Loss: 1.0743810314881175\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0.00/44.7M [00:00<?, ?B/s]#015 60%|█████▉    | 26.7M/44.7M [00:00<00:00, 280MB/s]#015100%|██████████| 44.7M/44.7M [00:00<00:00, 280MB/s]\u001b[0m\n",
      "\u001b[34m2021-12-20 07:42:24,952 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-12-20 07:42:38 Uploading - Uploading generated training model\n",
      "2021-12-20 07:43:12 Completed - Training job completed\n",
      "VanishingGradient: NoIssuesFound\n",
      "Overfit: NoIssuesFound\n",
      "Overtraining: NoIssuesFound\n",
      "PoorWeightInitialization: IssuesFound\n",
      "LossNotDecreasing: NoIssuesFound\n",
      "LowGPUUtilization: NoIssuesFound\n",
      "ProfilerReport: IssuesFound\n",
      "Training seconds: 1487\n",
      "Billable seconds: 1487\n"
     ]
    }
   ],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"train_model.py\",\n",
    "    base_job_name=\"smdebugger-dogimage-breed-classification\",\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    "    ## Debugger and Profiler parameters\n",
    "    rules=rules,\n",
    "    debugger_hook_config=hook_config,\n",
    "    profiler_config=profiler_config,\n",
    ")\n",
    "estimator.fit(inputs= input_paths ,wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/debug-output\n"
     ]
    }
   ],
   "source": [
    "print(estimator.latest_job_debugger_artifacts_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-19 12:00:40.841 datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:20 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-12-19 12:00:40.857 datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:20 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-19-11-24-55-724/debug-output\n",
      "[2021-12-19 12:00:41.304 datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:20 INFO trial.py:198] Training has ended, will refresh one final time in 1 sec.\n",
      "[2021-12-19 12:00:42.328 datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:20 INFO trial.py:210] Loaded all steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CrossEntropyLoss_output_0',\n",
       " 'gradient/ResNet_fc.0.bias',\n",
       " 'gradient/ResNet_fc.0.weight',\n",
       " 'layer1.0.relu_input_0',\n",
       " 'layer1.0.relu_input_1',\n",
       " 'layer1.1.relu_input_0',\n",
       " 'layer1.1.relu_input_1',\n",
       " 'layer2.0.relu_input_0',\n",
       " 'layer2.0.relu_input_1',\n",
       " 'layer2.1.relu_input_0',\n",
       " 'layer2.1.relu_input_1',\n",
       " 'layer3.0.relu_input_0',\n",
       " 'layer3.0.relu_input_1',\n",
       " 'layer3.1.relu_input_0',\n",
       " 'layer3.1.relu_input_1',\n",
       " 'layer4.0.relu_input_0',\n",
       " 'layer4.0.relu_input_1',\n",
       " 'layer4.1.relu_input_0',\n",
       " 'layer4.1.relu_input_1',\n",
       " 'relu_input_0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())\n",
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a debugging output.\n",
    "def get_data(trial, tname, mode):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps(mode=mode)\n",
    "    vals = []\n",
    "    for s in steps:\n",
    "        vals.append(tensor.value(s, mode=mode))\n",
    "    return steps, vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor(trial, tensor_name):\n",
    "\n",
    "    steps_train, vals_train = get_data(trial, tensor_name, mode=ModeKeys.TRAIN)\n",
    "    print(\"loaded TRAIN data\")\n",
    "    steps_eval, vals_eval = get_data(trial, tensor_name, mode=ModeKeys.EVAL)\n",
    "    print(\"loaded EVAL data\")\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    host = host_subplot(111)\n",
    "\n",
    "    par = host.twiny()\n",
    "\n",
    "    host.set_xlabel(\"Steps (TRAIN)\")\n",
    "    par.set_xlabel(\"Steps (EVAL)\")\n",
    "    host.set_ylabel(tensor_name)\n",
    "\n",
    "    (p1,) = host.plot(steps_train, vals_train, label=tensor_name)\n",
    "    print(\"completed TRAIN plot\")\n",
    "    (p2,) = par.plot(steps_eval, vals_eval, label=\"val_\" + tensor_name)\n",
    "    print(\"completed EVAL plot\")\n",
    "    leg = plt.legend()\n",
    "\n",
    "    host.xaxis.get_label().set_color(p1.get_color())\n",
    "    leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "    par.xaxis.get_label().set_color(p2.get_color())\n",
    "    leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "    plt.ylabel(tensor_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded TRAIN data\n",
      "loaded EVAL data\n",
      "completed TRAIN plot\n",
      "completed EVAL plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tensor(trial, \"CrossEntropyLoss_output_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?\n",
    "\n",
    "I am not sure if there is any anomolous behavior in my training, but it would seem loss has decreased. Also, there could be overfitting that could be resolved by early stopping training, there could be problem with loss not decreasing in whihc case, I think, more hyperparameter tuning is required, there could be poor weight initialization which, I think, could be resolved using a different model sicne we are fine tuning the model by freezing all the layers except for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will find the profiler report in s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output\n"
     ]
    }
   ],
   "source": [
    "# TODO: Display the profiler output\n",
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-20 07:42:47     375252 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-report.html\n",
      "2021-12-20 07:42:46     223182 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-report.ipynb\n",
      "2021-12-20 07:42:41        192 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/BatchSize.json\n",
      "2021-12-20 07:42:41        200 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json\n",
      "2021-12-20 07:42:41       2035 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/Dataloader.json\n",
      "2021-12-20 07:42:41        127 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json\n",
      "2021-12-20 07:42:41        199 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json\n",
      "2021-12-20 07:42:41        119 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json\n",
      "2021-12-20 07:42:41        151 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json\n",
      "2021-12-20 07:42:41        231 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json\n",
      "2021-12-20 07:42:41       1228 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n",
      "2021-12-20 07:42:41        476 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json\n",
      "2021-12-20 07:42:41       2122 smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/StepOutlier.json\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {rule_output_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/Dataloader.json to ProfilerReport/profiler-output/profiler-reports/Dataloader.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json to ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/BatchSize.json to ProfilerReport/profiler-output/profiler-reports/BatchSize.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json to ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json to ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json to ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-report.ipynb to ProfilerReport/profiler-output/profiler-report.ipynb\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json to ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json to ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json to ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/StepOutlier.json to ProfilerReport/profiler-output/profiler-reports/StepOutlier.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json to ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json\n",
      "download: s3://sagemaker-us-east-1-015775941522/smdebugger-dogimage-breed-classificatio-2021-12-20-07-15-26-056/rule-output/ProfilerReport/profiler-output/profiler-report.html to ProfilerReport/profiler-output/profiler-report.html\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp {rule_output_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get the autogenerated folder name of profiler report\n",
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ProfilerReport'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler_report_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-12-19 13:42:35 Starting - Preparing the instances for training\n",
      "2021-12-19 13:42:35 Downloading - Downloading input data\n",
      "2021-12-19 13:42:35 Training - Training image download completed. Training in progress.\n",
      "2021-12-19 13:42:35 Uploading - Uploading generated training model\n",
      "2021-12-19 13:42:35 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "training_job_name = f'smdebugger-dogimage-breed-classificatio-2021-12-19-13-16-59-023'\n",
    "estimator = PyTorch.attach(training_job_name)\n",
    "estimator.entry_point = 'train_model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_model.py'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.entry_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f'smdebugger-dogimage-breed-classificatio-2021-12-17-06-18-38-081'\n",
    "predictor = PyTorchPredictor(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# TODO: Deploy your model to an endpoint\n",
    "\n",
    "predictor=estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\") # TODO: Add your deployment configuration like instance type and number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run an prediction on the endpoint\n",
    "import os\n",
    "import boto3\n",
    "import io\n",
    "import numpy as np\n",
    "# import cv2\n",
    "from PIL import Image\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "def image_from_s3(bucket, key):\n",
    "\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    image = bucket.Object(key)\n",
    "    img_data = image.get().get('Body').read()\n",
    "\n",
    "    return Image.open(io.BytesIO(img_data))\n",
    "\n",
    "object_key = 'dogImages/test/007.American_foxhound/American_foxhound_00512.jpg'\n",
    "img = image_from_s3(bucket, object_key)# TODO: Your code to load and preprocess image to send to endpoint for prediction\n",
    "# open_cv_image = np.array(image) \n",
    "# image = cv2.resize(open_cv_image, (224,224))\n",
    "# display(open_cv_image)\n",
    "img = img.resize((224, 224), Image.ANTIALIAS)\n",
    "print(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.entry_point=\"train_model.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [224, 224, 3] instead\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 126, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 216, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 125, in default_predict_fn\n    output = model(input_data)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\", line 249, in forward\n    return self._forward_impl(x)\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\", line 232, in _forward_impl\n    x = self.conv1(x)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 399, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 396, in _conv_forward\n    self.padding, self.dilation, self.groups)\nRuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [224, 224, 3] instead\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/smdebugger-dogimage-breed-classificatio-2021-12-20-07-45-36-672 in account 015775941522 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7497a0e101a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"solution:  https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [224, 224, 3] instead\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 126, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py\", line 216, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File \"/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 125, in default_predict_fn\n    output = model(input_data)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\", line 249, in forward\n    return self._forward_impl(x)\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\", line 232, in _forward_impl\n    x = self.conv1(x)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 399, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 396, in _conv_forward\n    self.padding, self.dilation, self.groups)\nRuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [224, 224, 3] instead\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/smdebugger-dogimage-breed-classificatio-2021-12-20-07-45-36-672 in account 015775941522 for more information."
     ]
    }
   ],
   "source": [
    "\"solution:  https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html\"\n",
    "response = predictor.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-ef8b248111bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
